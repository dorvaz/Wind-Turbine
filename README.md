# Wind-Turbine
Wind Turbine Production Prediction
https://www.kaggle.com/berkerisen/wind-turbine-scada-dataset

## Context

In Wind Turbines, Scada Systems measure and save data's like wind speed, wind direction, 
generated power etc. for 10 minutes intervals. This file was taken from a wind turbine's 
scada system that is working and generating power in Turkey.

## Content

The data's in the file are:

*	**Date/Time**: (for 10 minutes intervals).
*	**LV ActivePower (kW)**: The power generated by the turbine for that moment.
*	**Wind Speed (m/s)**: The wind speed at the hub height of the turbine (the wind speed that turbine use for electricity generation)
*	**TheoreticalPowerCurve (KWh)**: The theoretical power values that the turbine generates with that wind speed which is given by the turbine manufacturer.
*	**Wind Direction (°)**: The wind direction at the hub height of the turbine (wind turbines turn to this direction automaticly).

## Problem definition

The objective of this kernel is the prediction of the energy produced by a wind turbine. For this estimation, we use the data provided in the dataset: wind speed and direction, generated power, and theoretical power that should produce (data provided by the manufacturer).

This is a supervised learning problem, since they provide us with the target variable.
It should be noted that they do not provide us with some relevant data, such as: location, temperature, humidity, rain, air density, technical stops, breakdowns...


## Used packages:

* **numpy:** A library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.
* **pandas:** A fast, powerful, flexible and easy-to-use open source data analysis and manipulation tool.
* **os:** The OS module in Python provides a way of using operating system dependent functionality. The functions that the OS module provides allows you to interface with the underlying operating system that Python is running on – be that Windows, Mac or Linux.
* **warnings:** Python programmers issue warnings by calling the warn() function defined in this module. 
* **time:** The Python time module provides many ways of representing time in code, such as objects, numbers, and strings. It also provides functionality other than representing time, like waiting during code execution and measuring the efficiency of your code.
* **datetime:** The datetime module supplies classes for manipulating dates and times in both simple and complex ways. While date and time arithmetic is supported, the focus of the implementation is on efficient attribute extraction for output formatting and manipulation. For related functionality, see also the time and calendar modules.
* **scipy:** Python-based ecosystem of open-source software for mathematics, science, and engineering.
* **sklearn:** is a free machine learning library for Python. It features various algorithms like support vector machine, random forests, and k-neighbours, and it also supports Python numerical and scientific libraries like NumPy and SciPy .
* **xgboost:** open source library providing a hight-performance implementation of gradient boosted decision trees.
* **lightgbm:** is a gradient enhancement framework that uses a tree-based learning algorithm. Light GBM grows tree vertically while another algorithm grows tree horizontally, which means that Light GBM grows tree in leaf form while another algorithm grows in level. It will choose the leaf with the maximum delta loss to grow. By growing the same leaf, the smart leaf algorithm can reduce more losses than a smart level algorithm.
* **tensorflow:** It is an open source artificial intelligence library, using data flow graphs to build models. It allows developers to create large-scale neural networks with many layers. TensorFlow is mainly used for: Classification, Perception, Understanding, Discovering, Prediction and Creation.

* **matplotlib:** A Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. 
* **seaborn:** statistical data visualization. Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. For a brief introduction to the ideas behind the library, you can read the introductory notes.
* **windrose:** graphic tool used by meteorologists to give a succinct view of how wind speed and direction are typically distributed at a particular location. It can also be used to describe air quality pollution sources.


## Features
Regardless of the features provided in the dataset, we can create others:

**suitable wind speed:** Wind turbines start operating when the wind reaches a speed of 3 to 4 meters per second, and reaches maximum electricity production with a wind of about 13 to 14 meters per second. If the wind is very strong, for example 25 meters per second as average speed for 10 minutes, the wind turbines stop for safety reasons.

**loss:** difference between the Teorical Powercurve and ActivePower.

**efficient use hours:** hours of use with efficient wind.

We have only used these columns to illustrate the operation and behaviour of wind turbines.

We have not used them to make the predictions since they are highly correlated with those provided in the dataset: they are linear representations of the previous ones.


## Algorithms

To train this model there are some better algorithms than others, however since the amount of data is reduced we have used a lot of them to make the predictions, mainly based on trees and gradient decreasing:

* **LinearRegression:** fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.
* **ElasticNet:** Linear regression with combined L1 and L2 priors as regularizer.
* **Lasso:** Technically the Lasso model is optimizing the same objective function as the Elastic Net with l1_ratio=1.0
* **BayesianRidge:** Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand.
* **LassoLarsIC:** regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features.
* **RandomForestRegressor:** A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.
* **GradientBoostingRegressor:** Gradient Tree Boosting or Gradient Boosted Decision Trees (GBDT) is a generalization of boosting to arbitrary differentiable loss functions. GBDT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems in a variety of areas including Web search ranking and ecology.
* **ExtraTreesRegressor:** randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule.
* **KernelRidge:** Kernel ridge regression (KRR) [M2012] combines Ridge regression and classification (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.
* The form of the model learned by KernelRidge is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses -insensitive loss, both combined with l2 regularization. In contrast to SVR, fitting KernelRidge can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for , at prediction-time.
* **DecisionTreeRegressor:** are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.
* **KNeighborsRegressor:** Neighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based on the mean of the labels of its nearest neighbors.
* **XGBRegressor:**  is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) ... A wide range of applications: Can be used to solve regression, classification, ranking, and user-defined prediction problems
* **LGBMRegressor:** machine Learning is the fastest growing field in the world. Everyday there will be a launch of bunch of new algorithms, some of those fails and some achieve the peak of success. Today, I am touching one of the most successful machine learning algorithm, Light GBM.
* **SVR:** The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target.
* **Nural Networks** are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems "learn" to perform tasks by considering examples, generally without being programmed with task-specific rules. 
* **arima:** ‘Auto Regressive Integrated Moving Average’ is actually a class of models that ‘explains’ a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values.
* **RNN:** recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. ... Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs.


## Conclusions
R2-score in test set: 
* ScaledLR : 0.85492
* ScaledLASSO: 0.85501
* ScaledEN: 0.80315
* ScaledGBMR: 0.78462
* ScaledCART: 0.68208
* ScaledGBO: 0.81525
* ScaledXGBR: 0.79058
* ScaledKNN: 0.78632
* SaledSVM: 0.84633

* LGBMRegressor: 0.9405
* Deep Neural Network: 0.9119
* Arima (Predict 3-hours-ahead): 0.912

The best results are provided using LGMBregressor and Neural Networks. 

## Futher Improvements
* Use historical data with exogenous input to predict 3-hours ahead.
* Use wavelet trasnform to decompose the input signal in differnt frequencies.
* Try more complex deep-learning architectures like as LSTM, ConvLSTM, autoencoders, GANs. 
